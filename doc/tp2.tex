\documentclass[12pt]{article}
\usepackage{sbc-template}
\usepackage{graphicx}
\usepackage[lofdepth,lotdepth]{subfig}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{times,amsmath,epsfig}
\usepackage{amssymb}
\usepackage{url}
\usepackage{multirow}
\usepackage{array}
 \makeatletter
 \newif\if@restonecol
 \makeatother
 \let\algorithm\relax
 \let\endalgorithm\relax
\usepackage{listings}
\usepackage{float}
\usepackage[lined,algonl,ruled]{algorithm2e}
\usepackage{multirow}
\usepackage[brazil]{babel}
\usepackage[latin1]{inputenc}
\usepackage{enumitem}



% \setlist{nolistsep}

\sloppy

\title{Mineração de Dados: Trabalho Prático 2}

\author{Artur Rodrigues}

\address{Departamento de Ciência da Computação \\ Universidade Federal de Minas Gerais (UFMG)
    \email{artur@dcc.ufmg.br}
}

\begin{document}

\maketitle

\section{INTRODUÇÃO}

Problemas de agrupamento podem surgir de diversas aplicações, como mineração de dados e aprendizado de máquina, compressão de dados e classificação e reconhecimento de padrões. A noção do que constitui um bom agrupamento depende diretamente da aplicação e existem muitas maneiras de achar esses agrupamentos de acordo com diversos critérios, sejam eles \textit{ad hoc} ou sistemáticos.


\section{$K$-MEANS}

Dentre as formulações de agrupamentos fundamentadas na minimização de uma função objetivo, talvez a mais amplamente utilizada e estudada seja o agrupamento \textit{k}-means. Dado um conjunto de \textit{n} pontos num espaço real \textit{d}-dimensional, $\mathbb{R}^{d}$, e um inteiro $k$, o problema é determinar um conjunto de $k$ pontos em $\mathbb{R}^{d}$, denominados centros, para minimizar a distância total quadrada de cada ponto para seu centro mais próximo. Esse tipo de agrupamento é enquadrado na categoria geral de agrupamentos baseados em variância \cite{Inaba:1994:AWV:177424.178042, Inaba:1996:ERR:237218.237406}.

Agrupamento baseada no $k$-means está relacionada com uma séria de outras técnicas de agrupamentos e problemas de localização, incluiindo o \textit{k-medians} Euclidiano, no qual o objetivo é minimizar a soma das distâncias para o centro mais próximo, e tambem o problema \textit{k-centers}, onde o objetivo é minimizar a distância máxima de todos os pontos para o centro mais próximo.

Não existe solução eficiente para nenhum desses problemas e algumas formulações são NP-hard \cite{Garey:1979:CIG:578533}. Uma das heurísticas mais populares para a solução do $k$-means é baseada em um simples esquema iterativo que nusca solução mínimas locais. Esse algortimo é geralmente chamado de \textit{algoritmo k-means} \cite{Forgy65, MacQueen67}, e especificamente nesse trabalho é utilizada a versão conhecida como \textit{Algoritmo de Lloyd} \cite{Lloyd82leastsquares}.

O algoritmo de Lloyd é baseado na simples observação de que o posicionamento ótimo de um centro é no centróide do agrupamento associado. Dado um conjunto de $k$ centros $Z$, para cada centro $z \in Z$, seja $V(z)$ o conjunto de pontos onde $z$ é o vizinho mais próximo. Em termos geométricos, $V(z)$ é o conjunto de pontos sobre a célula de Voronoi de $z$ \cite{Preparata:1985:CGI:4333}. Cada estágio do algoritmo de LLoyd move cada ponto central $z$ para o centróide de $V(z)$ e depois atualiza $V(z)$ ao recomputar a distância de cada ponto para o seu centro mais próximo. Esses passos são repetidos até que uma condição de convergência seja atingida. Em geral, especialmente se nenhum ponto é equidistante de dois centro, o algoritmo irá eventualmente convergir para um ponto que é um mínimo local para a distorção. Nesse trabalho a condição de parada é satisfeita quando $V(z)$ para cada ponto central $z$ não se altera após um novo estágio de atualização.


\subsection{Complexidade}

Em termos de complexidade temporal, podemos dizer que a maior parte do tempo é gasta na computação das distâncias entre as observações e os centros. Essa operação tem custo $O(M)$, onde $M$ é a dimensão dos vetores. O passo de atualização computa $KN$ distâncias, dessa maneira, sua complexidade é $O(KNM)$. Para um número fixo de iterações $I$, a complexidade geral é $O(IKNM)$.

Dessa maneira, o $k$-means é linear em todos os fatores relevantes: iterações, número de agrupamentos, número de observações e dimensionalidade do espaço. Em \cite{Inaba:1994:AWV:177424.178042} é mostrado que se a dimensionalidade $M$ e o número de agrupamentos $K$ são fixados, o problema pode ser resolvido em $O(N^{MK+1}\log N)$.


\section{ESCOLHA DOS CENTRÓIDES INICIAIS}

\label{initialization}
Pode-se argumentar que o algoritmo $k$-means define um mapeamento determinístico a partir de um ponto inicial até a solução. Isso significa que o ótimo local encontrado como solução é sensível a escolha inicial dos agrupamentos. Arranjos completamente diferentes para a solução final podem surgir a partir de pequenas alterações na escolha incial. De acordo com \cite{DudaHartCV} (p. 228):

\hyphenation{start-ing}
\begin{quote}
    ``One question that plagues all hill-climbing procedures is the choice of the starting point. Unfortunately, there is no simple, universally good solution to this problem.''
\end{quote}

``Repetição com diferentes escolhas aleatórias'' \cite{DudaHartCV} é geralmente a estratégia mais utilizada. Nesse trabalho, foram avaliadas três diferentes maneiras de se escolher aleatoriamente os $k$ pontos iniciais para serem centros dos agrupamentos. Além disso, foi implementada uma maneira de se utilizar $k$ observações definidas pelo usuário como centróides iniciais.

\subsection{$k$ Centróides Aleatórios}

Como será apresentado na seção \ref{base}, cada uma das observações e consequentemente também cada um dos centróides é definido como um ponto em $[0, 1]^{M}$.

Assim, essa maneira de inicialização simplesmente define um valor real aleatório no intervalo $[0, 1]$ para cada uma das $M$ dimensões, para cada um dos vetores. Um exemplo de vetor gerado para $M = 5$ é apresentado abaixo:

\begin{align*}
    v = [0.43, 0.11, 0.78, 0.91, 0.32]
\end{align*}


\subsection{Inicialização de Forgy}

Nesse método, apresentado em \cite{Forgy65}, são escolhidas $k$ observações aleatórias para serem os os pontos centrais iniciais.

\subsection{Partições Aleatórias}

Essa alternativa assinala um agrupamento aleatório para cada uma das observações e em seguida procede para o passo de atualização do algoritmo $k$-means, obtendo pontos médios que serão utilizados como centróides iniciais.


\section{BASE DE DADOS}

\label{base}
A base de dados utilizada foi fornecida pela comissão avaliadora, onde cada observação é uma música identificada por um número, seguida de rótulos que foram associados a ela por usuários do sistema de onde foi extraída. Um exemplo de entrada é exibido abaixo:

\begin{center}
\verb|14 jazz music chillout futuristica electronic hip-hop|
\end{center}

Para a modelagem do problema foi efetuado o que é conhecido por desnormalização, onde cada um dos rótulos existentes na base de dados é representado por uma dimensão. No caso da base fornecida, existem $3869$ rótulos distintos, implicando no mapeamento do problema para $M = 3869$ dimensões.

Como consequência dessa modelagem, os pontos que representam cada uma das observações são vetores com alto grau de esparsidade: as posições que representam rótulos da observação assumem valor $1$ e as demais $0$.

A figure \ref{top_10_labels} exibe os 10 rótulos mais frequentes.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{../plots/top_10_labels.pdf}
    \caption{Os 10 rótulos mais frequentes}
    \label{top_10_labels}
\end{figure}


\section{AVALIAÇÃO EXPERIMENTAL}

\subsection{Procedimentos}

Com o intuito de se obter testes mais consistentes, os experimentos foram executados em ambiente virtualizado, com capacidade de processamento e memória primária reduzidas, 50\% da capacidade da máquina hospedeira e 1024MiB, respectivamente. O sistema operacional do ambiente virtualizado era Ubuntu Server 12.04 64 bits e os softwares utilizados foram interpretador Python (2.7.2) e GCC versão 4.2.1. A máquina hospedeira possuía sistema operacional Mac OS X 10.8.2, processador \textit{quad-core} de 2.3GHz e memória primária com capacidade de 16GiB.

Todos os testes foram realizados 3 vezes e o resultados médios para os valores aferidos foram considerados. Finalmente, certificou-se que a solução desenvolvida execute perfeitamente na estação \verb+claro.grad.dcc.ufmg.br+.


\subsection{Análise dos Métodos de Inicialização}

Como apresentado na seção \ref{initialization}, foram implementadas três maneiras de obter os centróides inciais. Cada uma dessas alternativas foi estudada, com valor de $k=50$, valor esse obtido através da regra de ouro $k \approx \sqrt{N/2}$ \cite{Mardia79}. A tabela \ref{tab:inicializacao} apresenta os resultados para essas execuções.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
    \begin{tabular}{p{2.4cm}|r|r|>{\raggedleft}p{2.5cm}|r}
        \toprule
        \textbf{Medidas} & \multicolumn{1}{c|}{\textbf{Valores}} & \multicolumn{1}{c|}{\textbf{Centróids Aleat.}} & \multicolumn{1}{c|}{\textbf{Forgy}} & \multicolumn{1}{c}{\textbf{Partições Aleat.}} \tabularnewline
        \midrule
        \multirow{4}[4]{*}{\textbf{Iterações}} & Exec. 1 & 57    & 13    & 19 \tabularnewline
              & Exec. 2 & 56    & 14    & 15 \tabularnewline
              & Exec. 3 & 61    & 17    & 22 \tabularnewline
              & \textbf{Média} & \textbf{58,00} & \textbf{14,67} & \textbf{18,67} \tabularnewline
        \hline
        \multirow{4}[4]{*}{\textbf{\vbox{Distância Quadrada Total}}} & Exec. 1 & 17903,03 & 17789,62 & 16923,46 \tabularnewline
              & Exec. 2 & 17989,05 & 17682,62 & 16926,98 \tabularnewline
              & Exec. 3 & 17793,68 & 17745,91 & 16906,91 \tabularnewline
              & \textbf{Média} & \textbf{17895,25} & \textbf{17739,38} & \textbf{16919,11} \tabularnewline
        \hline
        \multirow{4}[4]{*}{\textbf{Índice Jagota}} & Exec. 1 & 90,46 & 72,62 & 88,39 \tabularnewline
              & Exec. 2 & 89,77 & 63,91 & 87,20 \tabularnewline
              & Exec. 3 & 88,60 & 74,97 & 87,94 \tabularnewline
              & \textbf{Média} & \textbf{89,61} & \textbf{70,50} & \textbf{87,84} \tabularnewline
        \hline
        \multirow{4}[3]{*}{\textbf{Tempo}} & Exec. 1 & 721,09 & 164,63 & 249,11 \tabularnewline
              & Exec. 2 & 721,42 & 177,61 & 192,54 \tabularnewline
              & Exec. 3 & 796,81 & 219,69 & 291,86 \tabularnewline
              & \textbf{Média} & \textbf{746,44} & \textbf{187,31} & \textbf{244,50} \tabularnewline
        \bottomrule
    \end{tabular}%
  \caption{Comparação dos Métodos de Inicialização}
  \label{tab:inicializacao}%
\end{table}%

Percebe-se que o método de Inicialização de Forgy produz os agrupamentos com os melhores índices de Jagota, através de menos iterações e consequentemente em menos tempo. Observa-se ainda que o método Partições Aleatórias, apesar de não produzir um valor para o índice de Jagota tão bom quanto o de Forgy, produz os agrupamentos com a menor distância quadrada total.


\subsection{Análise do Valor de K}

Joelho (distancia total quadrada media, jagota)

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{../plots/k_values.pdf}
    \caption{``Elbow'' para o estudo dos valores de $k$}
    \label{k_values}
\end{figure}


\subsection{Análise do Tamanho da Entrada}


\subsection{Análise da Qualidade da Solução}

Smallest total squared distance
Jagota


\section{CONCLUSÃO}


\nocite{*}
\bibliographystyle{sbc}
\bibliography{bib}

\end{document}
